# SAM Nunchaku Backbone W4-only Quantization Configuration
# 4-bit weights only, no activation quantization

quant:
  # Calibration settings (not needed for weight-only)
  calib:
    num_samples: 0
    batch_size: 1

  # Weight quantization - 4-bit
  wgts:
    dtype: uint4                    # 4-bit unsigned integer
    zero_point: PostScale           # Zero point after scaling
    static: true                    # Static quantization for weights
    group_shapes:
      - [1, 128]                    # Per-channel with group size 128
    scale_dtypes:
      - torch.float16               # FP16 scales

    # GPTQ configuration for better accuracy
    enable_kernel_gptq: true
    kernel_gptq:
      damp_percentage: 0.01
      block_size: 128
      num_inv_tries: 250
      hessian_block_size: 512
      skips: []

    # Skip certain layers
    skip_patch_embed: true          # Don't quantize patch embedding
    skip_first_block: false
    skip_last_block: false
    skip_decoder: false

    # Dynamic range calibration (optional)
    enable_calib_range: false

  # No input activation quantization
  ipts:
    dtype: null

  # No output activation quantization
  opts:
    dtype: null

  # Rotation (disabled)
  rotation:
    enabled: false

  # Smooth quantization (disabled)
  smooth:
    enabled: false
